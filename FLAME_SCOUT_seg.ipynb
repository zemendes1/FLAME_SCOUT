{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for downloading files\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import csv\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "#  imports for the network\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading zip file...\n",
      "Downloading zip file...\n"
     ]
    }
   ],
   "source": [
    "# Get files in current working directory\n",
    "files = os.listdir()\n",
    "\n",
    "# This url points to the download of the .zip file for the masks\n",
    "masks_url = \"https://onedrive.live.com/download?resid=AF45414AB81A52DC%21103772&authkey=!AGJ4SJC5qkrxV1g\"\n",
    "\n",
    "# This url points to the download of the .zip file for the masks\n",
    "images_url = \"https://onedrive.live.com/download?resid=AF45414AB81A52DC%21103773&authkey=!APqTtJD3wiy7Tpc\"\n",
    "\n",
    "urls =  [masks_url, images_url]\n",
    "\n",
    "if \"Images\" not in files or \"Masks\" not in files:\n",
    "    for url in urls:\n",
    "        # Download the zip file\n",
    "        print(\"Downloading zip file...\")\n",
    "        u = urllib.request.urlopen(url)\n",
    "        data = u.read()\n",
    "        u.close()\n",
    "\n",
    "        zip_filename = \"\"\n",
    "        if url == masks_url:\n",
    "            # Specify the local filename for the downloaded zip file\n",
    "            zip_filename = 'Masks.zip'\n",
    "        elif url == images_url:\n",
    "            # Specify the local filename for the downloaded zip file\n",
    "            zip_filename = 'Images.zip'\n",
    "\n",
    "        with open(zip_filename, 'wb') as f:\n",
    "            f.write(data)\n",
    "\n",
    "        # Unzip the downloaded file\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            # Extract all contents to the current working directory\n",
    "            zip_ref.extractall()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_path,masks_path, transform):\n",
    "        self.data = []\n",
    "        self.image_path = image_path\n",
    "        self.masks_path = masks_path\n",
    "        self.img_dim=(512, 512)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_file_list = sorted(glob.glob(self.image_path + \"/*.jpg\"), key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "        self.masks_file_list = sorted(glob.glob(self.masks_path + \"/*.png\"), key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "        for i in range(0,len(self.image_file_list)):\n",
    "\n",
    "                img = cv2.imread(self.image_file_list[i])[..., ::-1]  # Convert BGR to RGB\n",
    "                img = cv2.resize(img, self.img_dim)\n",
    "                img_tensor = self.transform(img)\n",
    "\n",
    "                mask = cv2.imread(self.masks_file_list[i], cv2.IMREAD_UNCHANGED)\n",
    "                mask = mask * 255\n",
    "                mask = cv2.resize(mask, self.img_dim)\n",
    "                mask_tensor = self.transform(mask)\n",
    "\n",
    "                self.data.append([img_tensor, mask_tensor])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def get_data (self):\n",
    "        return self.data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, mask = self.data[index]\n",
    "        #return {'image': image, 'mask': mask}\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will calculate the area for each of the ground truth masks and save it an csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "image_path = \"Images\"\n",
    "mask_path = \"Masks\"\n",
    "output_csv_path = \"hot_pixel_counts.csv\"  # Specify the desired path for the CSV file\n",
    "\n",
    "img_dim = (512, 512)\n",
    "\n",
    "masks_file_list = sorted(glob.glob(mask_path + \"/*.png\"), key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "with open(output_csv_path, mode='w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['File Name', 'Hot Pixel Count'])  # Writing header to CSV\n",
    "\n",
    "    for mask_file in masks_file_list:\n",
    "        mask = cv2.imread(mask_file, cv2.IMREAD_UNCHANGED)\n",
    "        mask = cv2.resize(mask, img_dim)\n",
    "\n",
    "        # Count the number of hot pixels in the mask\n",
    "        hot_pixel_count = cv2.countNonZero(mask)\n",
    "\n",
    "        # Save in the CSV file with file number\n",
    "        csv_writer.writerow([mask_file, hot_pixel_count])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters in this block and they will update in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1590266/1991066211.py:4: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/FLAME-SCOUT/FLAME-SCOUT-seg/e/FLMS-7\n"
     ]
    }
   ],
   "source": [
    "# prepare neptune\n",
    "!pip install neptune\n",
    "import neptune\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"FLAME-SCOUT/FLAME-SCOUT-seg\",\n",
    "    # Anders\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4MGI4ZWI5Zi1kZjBhLTQ3YzQtYjU0Yy03NTMxMjhmNWZhYjYifQ==\",\n",
    "    \n",
    "    # JosÃ©\n",
    "    # api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjMGUxMzgzYi02NTkyLTQ2MDgtOGQxMy0wNGU2YTUzOWNlYzQifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "parameters = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"batch_size\": 32,\n",
    "    \"n_epochs\": 15,\n",
    "    \"learning_rate\": 0.001,\n",
    "}\n",
    "run[\"model/parameters\"] = parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"Images\"\n",
    "mask_path = \"Masks\"\n",
    "\n",
    "image_files = os.listdir(image_path)\n",
    "mask_files = os.listdir(mask_path)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "dataset = FireDataset(image_path,mask_path, transform)\n",
    "length = FireDataset.__len__(dataset)\n",
    "\n",
    "# Assuming 'length' is the total number of samples in your dataset\n",
    "length = len(dataset)\n",
    "\n",
    "# Define the sizes for training, validation, and testing sets\n",
    "train_size = int(0.8 * length)\n",
    "val_size = int(0.1 * length)\n",
    "test_size = length - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoader instances for training, validation, and testing\n",
    "batch_size = parameters[\"batch_size\"]  # You can adjust this based on your needs\n",
    "train_loader = DataLoader(torch.utils.data.Subset(dataset, train_data.indices), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(torch.utils.data.Subset(dataset, val_data.indices), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(torch.utils.data.Subset(dataset, test_data.indices), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print('Length of the Training set: '+str(len(train_data)))\n",
    "print('Length of the Validation set: '+str(len(val_data)))\n",
    "print('Length of the Testing set: '+str(len(test_data)))\n",
    "\n",
    "# Printing one image and mask\n",
    "\n",
    "# Assuming train_loader is your DataLoader\n",
    "data_iterator = iter(train_loader)\n",
    "batch = next(data_iterator)\n",
    "image = batch[0]  # Assuming you want to visualize the first image in the batch\n",
    "print(image.shape)\n",
    "mask = batch[1]  # Assuming 'mask' is the key for your mask in the batch and it's one-dimensional\n",
    "print(mask.shape)\n",
    "# Assuming the image and mask are in CHW format (channels, height, width)\n",
    "image = image[0].permute(1, 2, 0)  # Change to HWC for matplotlib\n",
    "\n",
    "# Convert the torch tensors to NumPy arrays\n",
    "image = image.numpy()\n",
    "mask = mask.numpy()\n",
    "\n",
    "# Display the image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image)\n",
    "plt.title('Image')\n",
    "\n",
    "# Display the mask\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mask[0][0], cmap='plasma')\n",
    "plt.title('Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3, 16, 32, 64, 128, 256)):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i + 1]) for i in range(len(chs) - 1)])\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(256, 128, 64, 32, 16)):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.chs = chs\n",
    "        self.upconvs = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i + 1], kernel_size=2, stride=2) for i in range(len(chs) - 1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i + 1]) for i in range(len(chs) - 1)])\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs) - 1):\n",
    "            x = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x = self.dec_blocks[i](x)\n",
    "        return x\n",
    "\n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(3, 16, 32, 64, 128, 256), dec_chs=(256, 128, 64, 32, 16), num_class=1, retain_dim=True, out_sz=(512, 512)):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = Encoder(enc_chs)\n",
    "        self.decoder = Decoder(dec_chs)\n",
    "        self.head = nn.Conv2d(dec_chs[-1], num_class, kernel_size=1)\n",
    "        self.retain_dim = retain_dim\n",
    "        self.out_sz = out_sz\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out = self.head(out)\n",
    "\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, self.out_sz)\n",
    "        return out\n",
    "\n",
    "model = UNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, num_epochs, lr, device):\n",
    "    # Move the model to the device\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()  # Record the start time of the epoch\n",
    "        model.train()\n",
    "        training_loss = 0.0\n",
    "\n",
    "        for images, masks in train_loader:\n",
    "\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs.squeeze(dim=1), masks.float().squeeze(dim=1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "\n",
    "        average_loss = training_loss / len(train_loader)\n",
    "        run[\"training/loss\"].log(average_loss)\n",
    "\n",
    "        # Save the model after training 1 epoch\n",
    "        torch.save(model.state_dict(), \"pytorch_model_{}.pth\".format(epoch+1))\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for val_images, val_masks in val_loader:\n",
    "                val_images, val_masks = val_images.to(device), val_masks.to(device)\n",
    "\n",
    "                val_outputs = model(val_images)\n",
    "\n",
    "                val_loss += criterion(val_outputs.squeeze(dim=1), val_masks.float().squeeze(dim=1)).item()\n",
    "\n",
    "            average_val_loss = val_loss / len(val_loader)\n",
    "            run[\"validation/loss\"].log(average_val_loss)\n",
    "\n",
    "            # Calculate Elapsed Time\n",
    "            epoch_end_time = time.time()  # Record the end time of the epoch\n",
    "            epoch_time = epoch_end_time - epoch_start_time  # Calculate the time taken for the epoch\n",
    "            epoch_time_str = \"{:0>2}:{:05.2f}\".format(int(epoch_time // 60), epoch_time % 60)\n",
    "\n",
    "            file_path = 'README.md'\n",
    "            with open(file_path, 'a') as file:\n",
    "                # Assuming you have the variables epoch, epochs, training_loss, training_accuracy, val_loss, and accuracy defined\n",
    "                content = f\"Epoch {epoch+1} ({epoch_time_str})- Training Loss: {average_loss:.4f} - Validation Loss: {average_val_loss:.4f}\"\n",
    "\n",
    "                # Print the content to the console\n",
    "                print(content)\n",
    "\n",
    "                # Write the content to the file\n",
    "                print(content, file=file)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Move the model to the device\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "train(model, train_loader, val_loader, num_epochs=parameters[\"n_epochs\"], lr=parameters[\"learning_rate\"], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"pytorch_model_30.pth\"\n",
    "if model_path in files :\n",
    "            # Load the model\n",
    "            model = UNet()\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # If the model was saved with DataParallel, remove the 'module' prefix from keys\n",
    "            state_dict = torch.load(model_path, map_location=device)\n",
    "            #print(state_dict)\n",
    "            if 'module' in list(state_dict.keys())[0]:\n",
    "                state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "\n",
    "            # Move the model to the device\n",
    "            model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print a Random Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def visualize_random_prediction(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Choose a random index from the test set\n",
    "    random_index = np.random.randint(0, len(test_loader.dataset))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the random image and mask\n",
    "        random_image, random_mask = test_loader.dataset[random_index]\n",
    "        random_image = random_image.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Make a prediction\n",
    "        prediction = torch.sigmoid(model(random_image)).cpu().numpy()\n",
    "\n",
    "    # Convert PyTorch tensors to numpy arrays\n",
    "    random_image_np = TF.to_pil_image(random_image.squeeze())\n",
    "    random_mask_np = TF.to_pil_image(random_mask.squeeze())\n",
    "    prediction_np = TF.to_pil_image(torch.tensor(prediction.squeeze()))\n",
    "\n",
    "    # Display the images\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(random_image_np)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(random_mask_np, cmap='plasma')\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(prediction_np, cmap='plasma')\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_random_prediction(model, train_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the Testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
