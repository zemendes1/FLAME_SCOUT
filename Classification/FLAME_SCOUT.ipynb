{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lBMWdx6a_8_"
   },
   "source": [
    "Make all the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNwHUIQ_GTES"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you need to download these packages\n",
    "# !pip install neptune\n",
    "# !pip install neptune torch torchvision\n",
    "\n",
    "import neptune\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "import fileinput\n",
    "\n",
    "#  imports for the network\n",
    "import torchvision, torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare neptune\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"FLAME-SCOUT/FLAME-SCOUT\",\n",
    "    # Anders\n",
    "    # api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4MGI4ZWI5Zi1kZjBhLTQ3YzQtYjU0Yy03NTMxMjhmNWZhYjYifQ==\",\n",
    "    \n",
    "    # José\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjMGUxMzgzYi02NTkyLTQ2MDgtOGQxMy0wNGU2YTUzOWNlYzQifQ==\",\n",
    ")  # your credentials\n",
    "\n",
    "parameters = {\n",
    "    \"activation\": \"relu\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"batch_size\": 32,\n",
    "    \"n_epochs\": 50,\n",
    "    \"momentum\": 0.1\n",
    "}\n",
    "run[\"model/parameters\"] = parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA3o2a1BbDBu"
   },
   "source": [
    "Download the Dataset for Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugo7mQFxD5Rx"
   },
   "outputs": [],
   "source": [
    "# Get files in current working directory\n",
    "files = os.listdir()\n",
    "\n",
    "if 'Dataset' not in files:\n",
    "\n",
    "  # This url points to the download of the .zip file for the full classification training\n",
    "    url = 'https://www.dropbox.com/scl/fi/9sxb3s88hw2zr2f0bbvf9/Dataset.zip?rlkey=8s4bobjz0b7ee68vjt384cjk1&dl=1'\n",
    "\n",
    "  # This url points to the download of the .zip file for the edited classification training\n",
    "  # url = 'https://www.dropbox.com/scl/fi/8agmlcmfwezfyi4rjq631/Dataset.zip?rlkey=gvhl191glm8tmevv6e375lp2d&dl=1'\n",
    "\n",
    "\n",
    "    # Download the zip file\n",
    "    u = urllib.request.urlopen(url)\n",
    "    data = u.read()\n",
    "    u.close()\n",
    "\n",
    "    # Specify the local filename for the downloaded zip file\n",
    "    zip_filename = 'Dataset.zip'\n",
    "\n",
    "    with open(zip_filename, 'wb') as f:\n",
    "        f.write(data)\n",
    "\n",
    "    # Unzip the downloaded file\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        # Extract all contents to the current working directory\n",
    "        zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIuSD1ONbJcH"
   },
   "source": [
    "Preprocessing and examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "executionInfo": {
     "elapsed": 1420,
     "status": "ok",
     "timestamp": 1700210254478,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "WBIXax-OB5sV",
    "outputId": "7dea0625-7f67-4254-d5a3-e5e4de31f5bc"
   },
   "outputs": [],
   "source": [
    "# Get the current folder paths of the dataset\n",
    "Training_path = 'Dataset/Classification/Training'\n",
    "Testing_path = 'Dataset/Classification/Test'\n",
    "\n",
    "# Create the datasets\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "training_dataset = torchvision.datasets.ImageFolder(Training_path, transform=transform)\n",
    "testing_dataset = torchvision.datasets.ImageFolder(Testing_path, transform=transform)\n",
    "\n",
    "# Split the training data (80%)\n",
    "training_len = len(training_dataset.samples)\n",
    "val_split = 0.2\n",
    "val_len = int(val_split * training_len )\n",
    "training_len -= val_len\n",
    "training_dataset, validation_dataset = torch.utils.data.random_split(training_dataset, [training_len , val_len])\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size_training = 32\n",
    "training_loader = DataLoader(training_dataset,batch_size=batch_size_training,shuffle=True,num_workers=0)\n",
    "\n",
    "batch_size_validation = 32\n",
    "validation_loader = DataLoader(validation_dataset,batch_size=batch_size_training,shuffle=True,num_workers=0)\n",
    "\n",
    "batch_size_testing = 32\n",
    "testing_loader = DataLoader(testing_dataset,batch_size=batch_size_testing,shuffle=True,num_workers=0)\n",
    "\n",
    "print('Length of the Training set: '+str(len(training_dataset)))\n",
    "print('Length of the Validation set: '+str(len(validation_dataset)))\n",
    "print('Length of the Testing set: '+str(len(testing_dataset)))\n",
    "\n",
    "map = {0:'Fire', 1:'No Fire'}\n",
    "\n",
    "# Print one image\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(image.numpy(), (1, 2, 0)))\n",
    "    plt.title(map[label.item()])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDQcROlDa7uA"
   },
   "source": [
    "Define the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700210177230,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "vqytLkJRaXcY",
    "outputId": "2e0ab466-b272-425f-ebd1-6df0a0bb9320"
   },
   "outputs": [],
   "source": [
    "class FLAME_SCOUT_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FLAME_SCOUT_Model, self).__init__()\n",
    "\n",
    "        # Initial Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        # Separable Convolution Blocks with Residual Connections\n",
    "        self.conv_blocks = self._make_conv_blocks(8)\n",
    "\n",
    "        # 1x1 Convolution for Residual Connection\n",
    "        self.conv_residual = nn.Conv2d(8, 8, kernel_size=1, stride=1, padding=0)  # Adjusted stride\n",
    "\n",
    "        # Final Convolutional Block\n",
    "        self.final_conv = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.bn_final = nn.BatchNorm2d(8)\n",
    "\n",
    "        # Global Average Pooling and Output Layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(8, num_classes)\n",
    "\n",
    "    def _make_conv_blocks(self, size):\n",
    "        return nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(size, size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(size, size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(size),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        previous_block_activation = x\n",
    "\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "            # Use 1x1 convolution for residual connection\n",
    "            residual = self.conv_residual(previous_block_activation)\n",
    "            residual = F.interpolate(residual, size=x.size()[2:], mode='nearest')  # Adjusted to match the spatial dimensions\n",
    "            x = x + residual\n",
    "            previous_block_activation = x\n",
    "\n",
    "        x = F.relu(self.bn_final(self.final_conv(x)))\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 4687654,
     "status": "error",
     "timestamp": 1700139867221,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "NiXOIW-JJ9HJ",
    "outputId": "33d0a4e8-f043-4fab-d064-6525f3234b51"
   },
   "outputs": [],
   "source": [
    "def train_pytorch_model(model, train_loader, val_loader, criterion, optimizer, epochs, starting_epoch, device, save_model_flag=False):\n",
    "\n",
    "    for epoch in range(starting_epoch-1, epochs+starting_epoch-1):\n",
    "        epoch_start_time = time.time()  # Record the start time of the epoch\n",
    "\n",
    "        #Initialize Accuracy Values\n",
    "        training_loss=0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            training_loss += loss.item()\n",
    "            run[\"train/batch/loss\"].append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            batch_number = total/32\n",
    "            if batch_number % 100 ==0:\n",
    "              print(\"This is batch {} of epoch {}\".format(batch_number,epoch+1))\n",
    "\n",
    "        training_loss /= len(train_loader)\n",
    "        training_accuracy = correct / total\n",
    "        run[\"valid/acc\"] = correct / total\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Save the model after training 1 epoch\n",
    "        if save_model_flag:\n",
    "          torch.save(model.state_dict(), \"pytorch_model_{}.pth\".format(epoch+1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        epoch_end_time = time.time()  # Record the end time of the epoch\n",
    "        epoch_time = epoch_end_time - epoch_start_time  # Calculate the time taken for the epoch\n",
    "        epoch_time_str = \"{:0>2}:{:05.2f}\".format(int(epoch_time // 60), epoch_time % 60)\n",
    " \n",
    "    \n",
    "        \n",
    "        file_path = 'README.md'\n",
    "        with open(file_path, 'a') as file:\n",
    "            # Assuming you have the variables epoch, epochs, training_loss, training_accuracy, val_loss, and accuracy defined\n",
    "            content = f\"Epoch {epoch+1} ({epoch_time_str})- Training Loss: {training_loss:.4f} - Training Accuracy: {100 * training_accuracy:.2f}% - Validation Loss: {val_loss:.4f} - Validation Accuracy: {100 * accuracy:.2f}%\"\n",
    "\n",
    "            # Print the content to the console\n",
    "            print(content)\n",
    "\n",
    "            # Write the content to the file\n",
    "            print(content, file=file)\n",
    "\n",
    "# Create an instance of the model\n",
    "num_classes = 2\n",
    "input_shape = (3, 254, 254)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Define loss function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "New_Run = True\n",
    "Pre_Trained = False\n",
    "\n",
    "# Load the model\n",
    "model = FLAME_SCOUT_Model(num_classes)\n",
    "\n",
    "if New_Run : \n",
    "  # Move the model to the device\n",
    "  model = nn.DataParallel(model)\n",
    "  model = model.to(device)\n",
    "\n",
    "  start_epoch = 1\n",
    "\n",
    "elif Pre_Trained :\n",
    "  # Define the starting model \n",
    "  model_name = \"pytorch_model_??.pth\"\n",
    "\n",
    "  # Load the module\n",
    "  state_dict = torch.load(model_name, map_location=device)\n",
    "  if 'module' in list(state_dict.keys())[0]:\n",
    "    state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "  model.load_state_dict(state_dict)\n",
    "\n",
    "  # Move the model to the device\n",
    "  model = nn.DataParallel(model)\n",
    "  model = model.to(device)\n",
    "\n",
    "  start_epoch = int(model_name.split(\"_\")[2].split(\".\")[0])+1\n",
    "\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "# Train the model\n",
    "train_pytorch_model(model, training_loader, validation_loader, criterion, optimizer, epochs, start_epoch, device, save_model_flag=True)\n",
    "\n",
    "run[\"model/parameters/n_epochs\"] = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXKVJ-69dshw"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('\\nEpoch {} Test Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch, correct, len(test_loader.dataset), accuracy))\n",
    "\n",
    "    return all_preds, all_targets, correct, len(test_loader.dataset),accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_to_line(file_path, line_number, text_to_add):\n",
    "    with fileinput.FileInput(file_path, inplace=True, backup='.bak') as file:\n",
    "        for i, line in enumerate(file, start=1):\n",
    "            if i == line_number:\n",
    "                print(line.rstrip() + text_to_add)\n",
    "            else:\n",
    "                print(line, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260428,
     "status": "ok",
     "timestamp": 1700210518164,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "n4NeP6-Ra8s_",
    "outputId": "77d2f25b-904d-473f-ae1a-70fefba7a525"
   },
   "outputs": [],
   "source": [
    "# Get files in current working directory\n",
    "files = os.listdir()\n",
    "\n",
    "# Check the dataset and the model are present in the files\n",
    "if 'Dataset' in files:\n",
    "    for i in range(start_epoch, start_epoch+epochs+1):\n",
    "        model_path = \"pytorch_model_{}.pth\".format(i)\n",
    "            \n",
    "        if model_path in files :\n",
    "            # Load the model\n",
    "            model = FLAME_SCOUT_Model(num_classes=2)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # If the model was saved with DataParallel, remove the 'module' prefix from keys\n",
    "            state_dict = torch.load(model_path, map_location=device)\n",
    "            if 'module' in list(state_dict.keys())[0]:\n",
    "                state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "\n",
    "            # Do the Testing\n",
    "            all_targets, all_preds,correct,total_number, accuracy= evaluate(model, testing_loader, i)\n",
    "\n",
    "            # Add the accuracy to the README.md file\n",
    "            file_path = 'README.md'\n",
    "            line_number = i  # Specify the line number where you want to add text\n",
    "            text_to_add = f\" - Test Accuracy: {correct}/{total_number} ({accuracy:.2f}%)\"\n",
    "            add_text_to_line(file_path, line_number, text_to_add)\n",
    "\n",
    "    # Calculate confusion matrix for the last iteration\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Fire\", \"No Fire\"], yticklabels=[\"Fire\", \"No Fire\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()    \n",
    "\n",
    "# Dataset or Model weren't found\n",
    "else:\n",
    "    print(\"Dataset or Model not found. Please download the dataset and unzip it to the current working directory.\")\n",
    "    \n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
