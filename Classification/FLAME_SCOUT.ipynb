{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lBMWdx6a_8_"
   },
   "source": [
    "Make all the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNwHUIQ_GTES"
   },
   "outputs": [],
   "source": [
    "# imports for downloading files\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "#  imports for the network\n",
    "import torchvision, torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA3o2a1BbDBu"
   },
   "source": [
    "Download the Dataset for Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugo7mQFxD5Rx"
   },
   "outputs": [],
   "source": [
    "# Get files in current working directory\n",
    "files = os.listdir()\n",
    "\n",
    "if 'Dataset' not in files:\n",
    "\n",
    "  # This url points to the download of the .zip file for the full classification training\n",
    "  # url = 'https://www.dropbox.com/scl/fi/9sxb3s88hw2zr2f0bbvf9/Dataset.zip?rlkey=8s4bobjz0b7ee68vjt384cjk1&dl=1'\n",
    "\n",
    "  # This url points to the download of the .zip file for the edited classification training\n",
    "  url = 'https://www.dropbox.com/scl/fi/8agmlcmfwezfyi4rjq631/Dataset.zip?rlkey=gvhl191glm8tmevv6e375lp2d&dl=1'\n",
    "\n",
    "\n",
    "  # Download the zip file\n",
    "  u = urllib.request.urlopen(url)\n",
    "  data = u.read()\n",
    "  u.close()\n",
    "\n",
    "  # Specify the local filename for the downloaded zip file\n",
    "  zip_filename = 'Dataset.zip'\n",
    "\n",
    "  with open(zip_filename, 'wb') as f:\n",
    "      f.write(data)\n",
    "\n",
    "  # Unzip the downloaded file\n",
    "  with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "      # Extract all contents to the current working directory\n",
    "      zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIuSD1ONbJcH"
   },
   "source": [
    "Preprocessing and examination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "executionInfo": {
     "elapsed": 1420,
     "status": "ok",
     "timestamp": 1700210254478,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "WBIXax-OB5sV",
    "outputId": "7dea0625-7f67-4254-d5a3-e5e4de31f5bc"
   },
   "outputs": [],
   "source": [
    "# Get the current folder paths of the dataset\n",
    "Training_path = 'Dataset/Classification/Training'\n",
    "Testing_path = 'Dataset/Classification/Test'\n",
    "\n",
    "# Create the datasets\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "training_dataset = torchvision.datasets.ImageFolder(Training_path, transform=transform)\n",
    "testing_dataset = torchvision.datasets.ImageFolder(Testing_path, transform=transform)\n",
    "\n",
    "# Split the training data (80%)\n",
    "training_len = len(training_dataset.samples)\n",
    "val_split = 0.2\n",
    "val_len = int(val_split * training_len )\n",
    "training_len -= val_len\n",
    "training_dataset, validation_dataset = torch.utils.data.random_split(training_dataset, [training_len , val_len])\n",
    "\n",
    "# Create the dataloaders\n",
    "batch_size_training = 32\n",
    "training_loader = DataLoader(training_dataset,batch_size=batch_size_training,shuffle=True,num_workers=0)\n",
    "\n",
    "batch_size_validation = 32\n",
    "validation_loader = DataLoader(validation_dataset,batch_size=batch_size_training,shuffle=True,num_workers=0)\n",
    "\n",
    "batch_size_testing = 32\n",
    "testing_loader = DataLoader(testing_dataset,batch_size=batch_size_testing,shuffle=True,num_workers=0)\n",
    "\n",
    "print('Length of the Training set: '+str(len(training_dataset)))\n",
    "print('Length of the Validation set: '+str(len(validation_dataset)))\n",
    "print('Length of the Testing set: '+str(len(testing_dataset)))\n",
    "\n",
    "map = {0:'Fire', 1:'No Fire'}\n",
    "\n",
    "# Print one image\n",
    "dataiter = iter(training_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "for image, label in zip(images, labels):\n",
    "    plt.figure()\n",
    "    plt.imshow(np.transpose(image.numpy(), (1, 2, 0)))\n",
    "    plt.title(map[label.item()])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDQcROlDa7uA"
   },
   "source": [
    "Define the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1700210177230,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "vqytLkJRaXcY",
    "outputId": "2e0ab466-b272-425f-ebd1-6df0a0bb9320"
   },
   "outputs": [],
   "source": [
    "class FLAME_SCOUT_Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(FLAME_SCOUT_Model, self).__init__()\n",
    "\n",
    "        # Initial Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "\n",
    "        # Separable Convolution Blocks with Residual Connections\n",
    "        self.conv_blocks = self._make_conv_blocks(8)\n",
    "\n",
    "        # 1x1 Convolution for Residual Connection\n",
    "        self.conv_residual = nn.Conv2d(8, 8, kernel_size=1, stride=1, padding=0)  # Adjusted stride\n",
    "\n",
    "        # Final Convolutional Block\n",
    "        self.final_conv = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.bn_final = nn.BatchNorm2d(8)\n",
    "\n",
    "        # Global Average Pooling and Output Layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(8, num_classes)\n",
    "\n",
    "    def _make_conv_blocks(self, size):\n",
    "        return nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(size, size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(size),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(size, size, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(size),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        previous_block_activation = x\n",
    "\n",
    "        for block in self.conv_blocks:\n",
    "            x = block(x)\n",
    "            # Use 1x1 convolution for residual connection\n",
    "            residual = self.conv_residual(previous_block_activation)\n",
    "            residual = F.interpolate(residual, size=x.size()[2:], mode='nearest')  # Adjusted to match the spatial dimensions\n",
    "            x = x + residual\n",
    "            previous_block_activation = x\n",
    "\n",
    "        x = F.relu(self.bn_final(self.final_conv(x)))\n",
    "\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 4687654,
     "status": "error",
     "timestamp": 1700139867221,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "NiXOIW-JJ9HJ",
    "outputId": "33d0a4e8-f043-4fab-d064-6525f3234b51"
   },
   "outputs": [],
   "source": [
    "def train_pytorch_model(model, train_loader, val_loader, criterion, optimizer, epochs, device, save_model_flag=False):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()  # Record the start time of the epoch\n",
    "\n",
    "        #Initialize Accuracy Values\n",
    "        training_loss=0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            training_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            batch_number = total/32\n",
    "            if batch_number % 100 ==0:\n",
    "              print(\"This is batch {} of epoch {}\".format(batch_number,epoch+1))\n",
    "\n",
    "        training_loss /= len(train_loader)\n",
    "        training_accuracy = correct / total\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Save the model after training 1 epoch\n",
    "        if save_model_flag:\n",
    "          torch.save(model.state_dict(), \"pytorch_model_{}.pth\".format(epoch+1))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        epoch_end_time = time.time()  # Record the end time of the epoch\n",
    "        epoch_time = epoch_end_time - epoch_start_time  # Calculate the time taken for the epoch\n",
    "        epoch_time_str = \"{:0>2}:{:05.2f}\".format(int(epoch_time // 60), epoch_time % 60)\n",
    " \n",
    "    \n",
    "        \n",
    "        file_path = 'output.txt'\n",
    "        with open(file_path, 'a') as file:\n",
    "            # Assuming you have the variables epoch, epochs, training_loss, training_accuracy, val_loss, and accuracy defined\n",
    "            content = f\"Epoch [{epoch+1}/{epochs}] ({epoch_time_str})- Training Loss: {training_loss:.4f} - Training Accuracy: {100 * training_accuracy:.2f}% - Validation Loss: {val_loss:.4f} - Validation Accuracy: {100 * accuracy:.2f}%\"\n",
    "\n",
    "            # Print the content to the console\n",
    "            print(content)\n",
    "\n",
    "            # Write the content to the file\n",
    "            print(content, file=file)\n",
    "\n",
    "# Create an instance of the model\n",
    "num_classes = 2\n",
    "input_shape = (3, 254, 254)\n",
    "model = FLAME_SCOUT_Model(num_classes)\n",
    "\n",
    "# Print the model architecture\n",
    "print(model)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# Move the model to the device\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the model\n",
    "train_pytorch_model(model, training_loader, validation_loader, criterion, optimizer, epochs, device, save_model_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXKVJ-69dshw"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    print('\\nEpoch {} Test Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch,\n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260428,
     "status": "ok",
     "timestamp": 1700210518164,
     "user": {
      "displayName": "José Miguel Mendes",
      "userId": "02413323273704565831"
     },
     "user_tz": -60
    },
    "id": "n4NeP6-Ra8s_",
    "outputId": "77d2f25b-904d-473f-ae1a-70fefba7a525"
   },
   "outputs": [],
   "source": [
    "# Get files in current working directory\n",
    "files = os.listdir()\n",
    "\n",
    "# Check the dataset and the model are present in the files\n",
    "if 'Dataset' in files:\n",
    "    for i in range(0, epochs):\n",
    "        model_path = \"pytorch_model_{}.pth\".format(i)\n",
    "        \n",
    "        if model_path in files :\n",
    "            # Load the model\n",
    "            model = FLAME_SCOUT_Model(num_classes=2)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "            # If the model was saved with DataParallel, remove the 'module' prefix from keys\n",
    "            state_dict = torch.load(model_path, map_location=device)\n",
    "            if 'module' in list(state_dict.keys())[0]:\n",
    "                state_dict = {k[7:]: v for k, v in state_dict.items()}\n",
    "\n",
    "            model.load_state_dict(state_dict)\n",
    "\n",
    "            # Do the Testing\n",
    "            evaluate(model, testing_loader, i)\n",
    "            \n",
    "\n",
    "# Dataset or Model weren't found\n",
    "else:\n",
    "    print(\"Dataset or Model not found. Please download the dataset and unzip it to the current working directory.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
